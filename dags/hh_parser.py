"""
SkillRadar ETL Pipeline - HeadHunter Vacancy Parser
====================================================
–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –°–±–æ—Ä –≤–∞–∫–∞–Ω—Å–∏–π —Å HH.ru API –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ –≤ Kafka
–ê–≤—Ç–æ—Ä: SkillRadar Team
–í–µ—Ä—Å–∏—è: 2.0.0
"""

import json
import os
import random
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Set
from dataclasses import dataclass, asdict

import pendulum
import requests
from kafka import KafkaProducer
from kafka.errors import KafkaError

from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from airflow.utils.log.logging_mixin import LoggingMixin

# ============================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ============================================

@dataclass
class Config:
    """–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞"""
    
    # –†–æ–ª–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞
    TARGET_ROLES: List[str] = None
    
    # ID –∫–æ–º–ø–∞–Ω–∏–π HH.ru
    TARGET_COMPANIES: List[int] = None
    
    # –†–µ–≥–∏–æ–Ω (113 = –†–æ—Å—Å–∏—è)
    AREA_ID: int = 113
    
    # Kafka –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    KAFKA_BOOTSTRAP_SERVERS: List[str] = None
    KAFKA_TOPIC: str = "raw_vacancies"
    
    # –õ–∏–º–∏—Ç—ã API
    MAX_PAGES_PER_QUERY: int = 5
    ITEMS_PER_PAGE: int = 20
    REQUEST_TIMEOUT: int = 10
    
    # Rate limiting (HH.ru –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è)
    DELAY_BETWEEN_PAGES: tuple = (0.3, 0.6)
    DELAY_DETAIL_REQUEST: float = 1.1  # –ú–∏–Ω–∏–º—É–º 1 —Å–µ–∫—É–Ω–¥–∞ –¥–ª—è –¥–µ—Ç–∞–ª–µ–π
    
    # Retry –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    MAX_RETRIES: int = 3
    RETRY_BACKOFF_FACTOR: int = 2
    
    # User-Agent –¥–ª—è API
    USER_AGENT: str = "SkillRadar/2.0 (academic_research; contact@skillradar.local)"
    
    def __post_init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        if self.TARGET_ROLES is None:
            self.TARGET_ROLES = [
                "–ê–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö",
                "Data Engineer",
                "Product Manager",
                "Java Developer",
                "QA Automation Engineer",
                "Python Developer",
                "DevOps Engineer"
            ]
        
        if self.TARGET_COMPANIES is None:
            self.TARGET_COMPANIES = [
                3529,   # –°–±–µ—Ä
                78638,  # –¢-–ë–∞–Ω–∫ (–¢–∏–Ω—å–∫–æ—Ñ—Ñ)
                1740,   # –Ø–Ω–¥–µ–∫—Å
                4181,   # Ozon
                39305,  # Wildberries
                3776,   # –ú–¢–°
                2180,   # VK
                1122,   # –ê–≤–∏—Ç–æ
            ]
        
        if self.KAFKA_BOOTSTRAP_SERVERS is None:
            kafka_servers = Variable.get(
                "KAFKA_BOOTSTRAP_SERVERS", 
                default_var=os.getenv("KAFKA_BOOTSTRAP_SERVERS", "kafka:29092")
            )
            self.KAFKA_BOOTSTRAP_SERVERS = [kafka_servers]

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥ –∏ logger
config = Config()
logger = LoggingMixin().log

# ============================================
# –ú–û–î–ï–õ–ò –î–ê–ù–ù–´–•
# ============================================

@dataclass
class VacancyData:
    """–ú–æ–¥–µ–ª—å –¥–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–∏ –¥–ª—è ClickHouse"""
    id: int
    name: str
    area_name: str
    employer_name: str
    published_at: str  # ISO 8601 —Ñ–æ—Ä–º–∞—Ç –¥–ª—è ClickHouse
    salary_from: Optional[float]
    salary_to: Optional[float]
    currency: Optional[str]
    key_skills: List[str]
    search_query: str
    
    def to_dict(self) -> Dict:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏"""
        return asdict(self)

# ============================================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ============================================

class APIRateLimitError(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ rate limit"""
    pass

class VacancyTransformer:
    """–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π"""
    
    @staticmethod
    def transform(raw_data: Dict, search_query: str) -> VacancyData:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö HH.ru –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        
        Args:
            raw_data: –°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç API
            search_query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (—Ä–æ–ª—å)
            
        Returns:
            VacancyData object
            
        Raises:
            ValueError: –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã
        """
        try:
            vacancy_id = int(raw_data['id'])
        except (KeyError, ValueError, TypeError) as e:
            raise ValueError(f"Invalid vacancy ID: {e}")
        
        # –ó–∞—Ä–ø–ª–∞—Ç–∞
        salary = raw_data.get('salary') or {}
        salary_from = salary.get('from')
        salary_to = salary.get('to')
        
        # –ù–∞–≤—ã–∫–∏
        skills = [
            skill['name'] 
            for skill in raw_data.get('key_skills', []) 
            if skill.get('name')
        ]
        
        # –í–ê–ñ–ù–û: published_at –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ —Å—Ç—Ä–æ–∫—É –≤ ISO 8601
        # ClickHouse —Å–∞–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç –µ—ë —á–µ—Ä–µ–∑ parseDateTime64BestEffort
        published_at = raw_data.get('published_at', '')
        
        return VacancyData(
            id=vacancy_id,
            name=raw_data.get('name', '').strip() or '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è',
            area_name=raw_data.get('area', {}).get('name', 'Unknown'),
            employer_name=raw_data.get('employer', {}).get('name', 'Unknown'),
            published_at=published_at,
            salary_from=float(salary_from) if salary_from is not None else None,
            salary_to=float(salary_to) if salary_to is not None else None,
            currency=salary.get('currency'),
            key_skills=skills,
            search_query=search_query
        )

class HHAPIClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å API HeadHunter"""
    
    def __init__(self, config: Config):
        self.config = config
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': config.USER_AGENT
        })
        
    def _request_with_retry(
        self, 
        url: str, 
        params: Optional[Dict] = None,
        max_retries: Optional[int] = None
    ) -> Optional[requests.Response]:
        """HTTP –∑–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–∞–º–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö"""
        max_retries = max_retries or self.config.MAX_RETRIES
        
        for attempt in range(max_retries):
            try:
                response = self.session.get(
                    url, 
                    params=params, 
                    timeout=self.config.REQUEST_TIMEOUT
                )
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ rate limit
                if response.status_code == 429:
                    retry_after = int(response.headers.get('Retry-After', 60))
                    logger.warning(
                        f"‚è≥ Rate limit reached. Waiting {retry_after}s... "
                        f"(Attempt {attempt + 1}/{max_retries})"
                    )
                    
                    if attempt < max_retries - 1:
                        time.sleep(retry_after)
                        continue
                    else:
                        raise APIRateLimitError(f"Rate limit exceeded after {max_retries} retries")
                
                response.raise_for_status()
                return response
                
            except requests.exceptions.RequestException as e:
                wait_time = self.config.RETRY_BACKOFF_FACTOR ** attempt
                
                if attempt == max_retries - 1:
                    logger.error(f"‚ùå Request failed after {max_retries} attempts: {e}")
                    return None
                
                logger.warning(
                    f"‚ö†Ô∏è Request failed (attempt {attempt + 1}/{max_retries}). "
                    f"Retrying in {wait_time}s... Error: {e}"
                )
                time.sleep(wait_time)
        
        return None
    
    def search_vacancies(
        self, 
        role: str, 
        company_id: int, 
        page: int = 0
    ) -> Optional[Dict]:
        """–ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ —Ä–æ–ª–∏ –∏ –∫–æ–º–ø–∞–Ω–∏–∏"""
        params = {
            'text': role,
            'employer_id': company_id,
            'area': self.config.AREA_ID,
            'per_page': self.config.ITEMS_PER_PAGE,
            'page': page
        }
        
        response = self._request_with_retry(
            'https://api.hh.ru/vacancies',
            params=params
        )
        
        return response.json() if response else None
    
    def get_vacancy_details(self, vacancy_id: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤–∞–∫–∞–Ω—Å–∏–∏"""
        response = self._request_with_retry(
            f'https://api.hh.ru/vacancies/{vacancy_id}'
        )
        
        return response.json() if response else None

class KafkaPublisher:
    """–ü—É–±–ª–∏–∫–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ Kafka"""
    
    def __init__(self, config: Config):
        self.config = config
        self.producer = None
        self._init_producer()
    
    def _init_producer(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Kafka producer"""
        try:
            self.producer = KafkaProducer(
                bootstrap_servers=self.config.KAFKA_BOOTSTRAP_SERVERS,
                value_serializer=lambda v: json.dumps(
                    v, ensure_ascii=False
                ).encode('utf-8'),
                acks='all',
                retries=3,
                max_in_flight_requests_per_connection=1,
                compression_type='gzip'
            )
            logger.info(
                f"‚úÖ Kafka producer initialized: "
                f"{self.config.KAFKA_BOOTSTRAP_SERVERS}"
            )
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize Kafka producer: {e}")
            raise
    
    def send(self, vacancy: VacancyData) -> bool:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ Kafka"""
        try:
            future = self.producer.send(
                self.config.KAFKA_TOPIC,
                value=vacancy.to_dict()
            )
            future.get(timeout=10)
            return True
            
        except KafkaError as e:
            logger.error(f"‚ùå Failed to send vacancy {vacancy.id} to Kafka: {e}")
            return False
    
    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ producer"""
        if self.producer:
            self.producer.flush()
            self.producer.close()
            logger.info("üîí Kafka producer closed")

# ============================================
# –û–°–ù–û–í–ù–ê–Ø –õ–û–ì–ò–ö–ê ETL
# ============================================

class VacancyCollector:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å —Å–±–æ—Ä—â–∏–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–π"""
    
    def __init__(self, config: Config):
        self.config = config
        self.api_client = HHAPIClient(config)
        self.kafka_publisher = KafkaPublisher(config)
        self.transformer = VacancyTransformer()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'processed_ids': set(),
            'sent_count': 0,
            'error_count': 0,
            'skipped_duplicates': 0,
            'start_time': None,
            'end_time': None
        }
    
    def collect(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å–±–æ—Ä–∞ –≤–∞–∫–∞–Ω—Å–∏–π"""
        self.stats['start_time'] = datetime.now()
        
        logger.info(
            f"üöÄ Starting vacancy collection:\n"
            f"   Roles: {len(self.config.TARGET_ROLES)}\n"
            f"   Companies: {len(self.config.TARGET_COMPANIES)}\n"
            f"   Max pages per query: {self.config.MAX_PAGES_PER_QUERY}"
        )
        
        try:
            for role in self.config.TARGET_ROLES:
                for company_id in self.config.TARGET_COMPANIES:
                    self._collect_for_role_company(role, company_id)
        finally:
            self.kafka_publisher.close()
            self._print_statistics()
    
    def _collect_for_role_company(self, role: str, company_id: int):
        """–°–±–æ—Ä –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ä–æ–ª–∏ –∏ –∫–æ–º–ø–∞–Ω–∏–∏"""
        logger.info(f"üîé Searching: '{role}' @ Company ID {company_id}")
        
        page = 0
        while page < self.config.MAX_PAGES_PER_QUERY:
            search_result = self.api_client.search_vacancies(
                role, company_id, page
            )
            
            if not search_result:
                logger.warning(f"‚ö†Ô∏è Failed to fetch page {page}")
                break
            
            items = search_result.get('items', [])
            total_pages = search_result.get('pages', 1)
            
            if not items or page >= total_pages:
                logger.debug(f"‚úì No more results at page {page}")
                break
            
            logger.debug(f"   Page {page + 1}/{total_pages}: {len(items)} vacancies")
            
            for item in items:
                self._process_vacancy(item, role)
            
            page += 1
            if page < total_pages:
                time.sleep(random.uniform(*self.config.DELAY_BETWEEN_PAGES))
    
    def _process_vacancy(self, item: Dict, role: str):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏"""
        try:
            vacancy_id = str(item['id'])
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            if vacancy_id in self.stats['processed_ids']:
                self.stats['skipped_duplicates'] += 1
                logger.debug(f"‚è≠Ô∏è  Skipping duplicate: {vacancy_id}")
                return
            
            self.stats['processed_ids'].add(vacancy_id)
            
            # Rate limiting –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            time.sleep(self.config.DELAY_DETAIL_REQUEST)
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            details = self.api_client.get_vacancy_details(vacancy_id)
            
            if not details:
                self.stats['error_count'] += 1
                logger.warning(f"‚ö†Ô∏è Failed to fetch details: {vacancy_id}")
                return
            
            # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
            try:
                vacancy_data = self.transformer.transform(details, role)
            except ValueError as e:
                self.stats['error_count'] += 1
                logger.warning(f"‚ö†Ô∏è Invalid data for {vacancy_id}: {e}")
                return
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Kafka
            if self.kafka_publisher.send(vacancy_data):
                self.stats['sent_count'] += 1
                
                if self.stats['sent_count'] % 50 == 0:
                    logger.info(
                        f"‚úÖ Progress: {self.stats['sent_count']} sent, "
                        f"{self.stats['error_count']} errors, "
                        f"{self.stats['skipped_duplicates']} duplicates"
                    )
            else:
                self.stats['error_count'] += 1
                
        except Exception as e:
            self.stats['error_count'] += 1
            logger.error(f"‚ùå Unexpected error processing vacancy: {e}")
    
    def _print_statistics(self):
        """–í—ã–≤–æ–¥ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        self.stats['end_time'] = datetime.now()
        duration = self.stats['end_time'] - self.stats['start_time']
        
        logger.info(
            f"\n"
            f"üèÅ Collection completed!\n"
            f"{'=' * 50}\n"
            f"Duration: {duration}\n"
            f"Total processed IDs: {len(self.stats['processed_ids'])}\n"
            f"‚úÖ Successfully sent: {self.stats['sent_count']}\n"
            f"‚è≠Ô∏è  Skipped duplicates: {self.stats['skipped_duplicates']}\n"
            f"‚ùå Errors: {self.stats['error_count']}\n"
            f"{'=' * 50}"
        )

# ============================================
# AIRFLOW TASK FUNCTIONS
# ============================================

def collect_vacancies(**context):
    """Airflow task function –¥–ª—è —Å–±–æ—Ä–∞ –≤–∞–∫–∞–Ω—Å–∏–π"""
    logger.info("=" * 60)
    logger.info("Starting SkillRadar ETL Pipeline")
    logger.info("=" * 60)
    
    try:
        collector = VacancyCollector(config)
        collector.collect()
        
        # –ü–µ—Ä–µ–¥–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ XCom
        context['ti'].xcom_push(
            key='collection_stats',
            value={
                'sent_count': collector.stats['sent_count'],
                'error_count': collector.stats['error_count'],
                'unique_vacancies': len(collector.stats['processed_ids']),
                'timestamp': datetime.now().isoformat()
            }
        )
        
        logger.info("‚úÖ ETL pipeline completed successfully")
        
    except Exception as e:
        logger.error(f"‚ùå ETL pipeline failed: {e}")
        raise

def verify_kafka_connection(**context):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Kafka –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º"""
    logger.info("üîç Verifying Kafka connection...")
    
    try:
        producer = KafkaProducer(
            bootstrap_servers=config.KAFKA_BOOTSTRAP_SERVERS,
            request_timeout_ms=5000
        )
        producer.close()
        logger.info("‚úÖ Kafka connection verified")
        
    except Exception as e:
        logger.error(f"‚ùå Kafka connection failed: {e}")
        raise

# ============================================
# AIRFLOW DAG DEFINITION
# ============================================

default_args = {
    'owner': 'skillradar',
    'depends_on_past': False,
    'start_date': pendulum.datetime(2025, 1, 1, tz='UTC'),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=3),
}

with DAG(
    dag_id='skillradar_hh_parser',
    default_args=default_args,
    description='SkillRadar: –ü–∞—Ä—Å–∏–Ω–≥ –≤–∞–∫–∞–Ω—Å–∏–π —Å HeadHunter API',
    schedule_interval='0 3 * * *',  # –ö–∞–∂–¥—ã–π –¥–µ–Ω—å –≤ 3:00 UTC
    catchup=False,
    max_active_runs=1,
    tags=['skillradar', 'etl', 'headhunter', 'production'],
    doc_md=__doc__,
) as dag:
    
    # Task 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ Kafka
    verify_kafka = PythonOperator(
        task_id='verify_kafka_connection',
        python_callable=verify_kafka_connection,
    )
    
    # Task 2: –û—Å–Ω–æ–≤–Ω–æ–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
    collect_data = PythonOperator(
        task_id='collect_hh_vacancies',
        python_callable=collect_vacancies,
    )
    
    # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    verify_kafka >> collect_data