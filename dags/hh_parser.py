from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from datetime import datetime, timedelta, timezone
import requests
import json
import logging
import time
import random
import re
import os
import backoff
from bs4 import BeautifulSoup
from kafka import KafkaProducer
from functools import lru_cache

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π settings.py
import settings

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
HH_API_VACANCIES = "https://api.hh.ru/vacancies"
KAFKA_TOPIC = "vacancies_enriched"
KAFKA_SERVERS = ['kafka:9092'] # –ò—Å–ø—Ä–∞–≤–∏–ª –æ–ø–µ—á–∞—Ç–∫—É

# User-Agent –¥–ª—è "–î–∑–µ–Ω" —Ä–µ–∂–∏–º–∞
HH_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'application/json'
}

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 1, 1),
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=24) # –î–∑–µ–Ω-—Ä–µ–∂–∏–º –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ–ª–≥–∏–º
}

dag = DAG(
    'skillradar_zen_v10',
    default_args=default_args,
    description='Zen Parser v10: Fixes, UTC Time, Regex Mining',
    schedule_interval='@daily',
    catchup=False,
    max_active_runs=1
)

# --- STATE MANAGEMENT (POSTGRES) ---
class StateManager:
    def __init__(self):
        self.hook = PostgresHook(postgres_conn_id='airflow_db')
        self._init_table()

    def _init_table(self):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º TIMESTAMP WITH TIME ZONE (TIMESTAMPTZ)
        sql = """
        CREATE TABLE IF NOT EXISTS parsing_state (
            employer_id BIGINT PRIMARY KEY,
            employer_name TEXT,
            last_published_at TIMESTAMPTZ, 
            updated_at TIMESTAMPTZ DEFAULT NOW()
        );
        """
        self.hook.run(sql)

    def get_last_date(self, emp_id):
        sql = "SELECT last_published_at FROM parsing_state WHERE employer_id = %s"
        row = self.hook.get_first(sql, parameters=(emp_id,))
        if row and row[0]:
            return row[0] # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç datetime —Å tzinfo (–æ–±—ã—á–Ω–æ UTC)
        return None

    def update_state(self, emp_id, name, last_pub_dt):
        sql = """
        INSERT INTO parsing_state (employer_id, employer_name, last_published_at, updated_at)
        VALUES (%s, %s, %s, NOW())
        ON CONFLICT (employer_id) DO UPDATE SET
            last_published_at = GREATEST(parsing_state.last_published_at, EXCLUDED.last_published_at),
            updated_at = NOW();
        """
        self.hook.run(sql, parameters=(emp_id, name, last_pub_dt))

# --- –£–¢–ò–õ–ò–¢–´ ---

@backoff.on_exception(backoff.expo, requests.exceptions.RequestException, max_tries=10)
def safe_request(url, params=None):
    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º –æ–∂–∏–¥–∞–Ω–∏–µ–º –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö"""
    r = requests.get(url, params=params, headers=HH_HEADERS, timeout=30)
    r.raise_for_status()
    return r

def clean_html(raw_html):
    if not raw_html: return ""
    return BeautifulSoup(raw_html, "html.parser").get_text(separator=" ").strip()

@lru_cache(maxsize=1)
def get_compiled_skills():
    """
    –ö–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç Regex –æ–¥–∏–Ω —Ä–∞–∑. 
    –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç ['go', 'java'] –≤ r'\b(go|java)\b' (—Å –≥—Ä–∞–Ω–∏—Ü–∞–º–∏ —Å–ª–æ–≤).
    """
    compiled = {}
    for group, skills in settings.SKILL_DICTIONARY.items():
        # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã (c++, c#)
        escaped_skills = [re.escape(s) for s in skills]
        # –°–æ–±–∏—Ä–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω: –≥—Ä–∞–Ω–∏—Ü—ã —Å–ª–æ–≤–∞ + (–∏–ª–∏|–∏–ª–∏) + –≥—Ä–∞–Ω–∏—Ü—ã
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º (?i) –¥–ª—è –∏–≥–Ω–æ—Ä–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞
        pattern = re.compile(r'\b(' + '|'.join(escaped_skills) + r')\b', re.IGNORECASE)
        compiled[group] = pattern
    return compiled

def deep_skill_mining(description_text):
    """–ü–æ–∏—Å–∫ –Ω–∞–≤—ã–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é Regex"""
    if not description_text: return []
    
    patterns = get_compiled_skills()
    found = set()
    
    for group, pattern in patterns.items():
        # findall –≤–µ—Ä–Ω–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        matches = pattern.findall(description_text)
        for m in matches:
            found.add(m.lower()) # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
            
    return list(found)

def categorize_and_filter(name):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–ª–∏ None (–µ—Å–ª–∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–æ).
    –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞, –ø–æ—Ç–æ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.
    """
    name_lower = name.lower()
    
    # 1. Stop-words (Regex search)
    for bad_regex in settings.STOP_WORDS:
        if re.search(bad_regex, name_lower, re.IGNORECASE):
            return None # Drop it
            
    # 2. Categories
    for cat, kws in settings.CATEGORIES_RULES.items():
        if any(k in name_lower for k in kws):
            return cat
            
    return 'Uncategorized' # Keep it

def random_sleep(min_s=1.0, max_s=3.0):
    time.sleep(random.uniform(min_s, max_s))

# --- –û–°–ù–û–í–ù–ê–Ø –ó–ê–î–ê–ß–ê ---
def run_parser_v10(**context):
    state = StateManager()
    producer = None
    
    try:
        producer = KafkaProducer(
            bootstrap_servers=KKA_SERVERS, # –í –∫–æ–¥–µ –≤—ã—à–µ –º—ã –∏—Å–ø—Ä–∞–≤–∏–ª–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É, –Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –∏–∑ imports
            # KafkaProducer —Ç—Ä–µ–±—É–µ—Ç —Å—Ç—Ä–æ–∫—É –∏–ª–∏ —Å–ø–∏—Å–æ–∫. –ò—Å–ø—Ä–∞–≤–∏–º –Ω–∞ KAFKA_SERVERS –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
    except NameError: 
         # –ï—Å–ª–∏ –≤–¥—Ä—É–≥ –∑–∞–±—ã–ª–∏ –ø–æ–ø—Ä–∞–≤–∏—Ç—å –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É, —Ö–∞—Ä–¥–∫–æ–¥ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
         producer = KafkaProducer(
            bootstrap_servers=['kafka:9092'],
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )

    logging.info(f"üßò‚Äç‚ôÇÔ∏è Zen Parser v10 Started. Companies: {len(settings.TARGET_COMPANIES)}")
    
    for emp_id, emp_name in settings.TARGET_COMPANIES.items():
        logging.info(f"üè¢ START: {emp_name} (ID: {emp_id})")
        
        # 1. WATERMARK
        last_date = state.get_last_date(emp_id)
        if last_date:
            # –í–∞–∂–Ω–æ: HH —Ç—Ä–µ–±—É–µ—Ç ISO —Ñ–æ—Ä–º–∞—Ç —Å —Ç–∞–π–º–∑–æ–Ω–æ–π. 
            # last_date –∏–∑ Postgres —É–∂–µ —Å TZ (UTC).
            date_from = last_date.isoformat()
            logging.info(f"   Delta load: > {date_from}")
        else:
            # 30 –¥–Ω–µ–π –Ω–∞–∑–∞–¥, —Å UTC —Ç–∞–π–º–∑–æ–Ω–æ–π
            date_from = (datetime.now(timezone.utc) - timedelta(days=30)).isoformat()
            logging.info(f"   First load: > {date_from} (30 days)")

        # 2. ITERATE
        page = 0
        total_pages = 1 # –ó–∞–≥–ª—É—à–∫–∞, –æ–±–Ω–æ–≤–∏—Ç—Å—è –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        max_pub_date = None
        new_count = 0
        
        while page < total_pages:
            params = {
                'employer_id': emp_id,
                'date_from': date_from,
                'per_page': 100,
                'page': page,
                'order_by': 'publication_time', # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
                'sort_order': 'asc'             # –û—Ç —Å—Ç–∞—Ä—ã—Ö –∫ –Ω–æ–≤—ã–º (—á—Ç–æ–±—ã –≤–∞—Ç–µ—Ä–º–∞—Ä–∫ —Ä–æ—Å –±–µ–∑–æ–ø–∞—Å–Ω–æ)
            }
            
            try:
                random_sleep(1.0, 2.5) # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –ª–∏—Å—Ç–∏–Ω–≥–æ–º
                resp = safe_request(HH_API_VACANCIES, params=params)
                data = resp.json()
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ –æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö
                total_pages = data.get('pages', 0)
                items = data.get('items', [])
                
                if not items:
                    break
                
                for item in items:
                    # –¢—Ä–µ–∫–∞–µ–º —Å–∞–º—É—é —Å–≤–µ–∂—É—é –¥–∞—Ç—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–µ–π—Ç–∞
                    # HH –æ—Ç–¥–∞–µ—Ç: "2023-10-05T12:00:00+0300"
                    pub_dt = datetime.fromisoformat(item['published_at'])
                    if not max_pub_date or pub_dt > max_pub_date:
                        max_pub_date = pub_dt

                    # –§–ò–õ–¨–¢–†–ê–¶–ò–Ø
                    category = categorize_and_filter(item.get('name', ''))
                    if not category:
                        continue # –°—Ç–æ–ø-–ª–∏—Å—Ç
                    
                    # –î–ï–¢–ê–õ–ò–ó–ê–¶–ò–Ø
                    random_sleep(0.5, 1.5) # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ —á—Ç–µ–Ω–∏–µ–º –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
                    
                    try:
                        full = safe_request(f"{HH_API_VACANCIES}/{item['id']}").json()
                        
                        desc_clean = clean_html(full.get('description', ''))
                        extracted = deep_skill_mining(desc_clean)
                        hh_keys = [s['name'] for s in full.get('key_skills', [])]
                        
                        # –°–æ–±–∏—Ä–∞–µ–º –ü–û–õ–ù–´–ô –ø–∞–∫–µ—Ç –¥–∞–Ω–Ω—ã—Ö (–∫–∞–∫ –ø—Ä–æ—Å–∏–ª–æ —Ä–µ–≤—å—é)
                        msg = {
                            'id': int(item['id']),
                            'employer_id': emp_id,
                            'employer_name': emp_name,
                            'url': item.get('alternate_url'),
                            'published_at': item['published_at'], # –û—Å—Ç–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫—É ISO
                            'name': item['name'],
                            'category': category,
                            
                            # –î–µ–Ω—å–≥–∏
                            'salary_from': (item.get('salary') or {}).get('from'),
                            'salary_to': (item.get('salary') or {}).get('to'),
                            'currency': (item.get('salary') or {}).get('currency'),
                            'gross': 1 if (item.get('salary') or {}).get('gross') else 0,
                            
                            # –ú–µ—Ç–∞
                            'experience_id': (full.get('experience') or {}).get('id'),
                            'schedule': (full.get('schedule') or {}).get('name'),
                            'employment': (full.get('employment') or {}).get('name'),
                            'area_name': (full.get('area') or {}).get('name'),
                            
                            # –°–∫–∏–ª–ª—ã
                            'key_skills': hh_keys,
                            'extracted_skills': extracted,
                            'description': desc_clean
                        }
                        
                        producer.send(KAFKA_TOPIC, msg)
                        new_count += 1
                        
                    except Exception as e:
                        logging.error(f"Error fetching details {item['id']}: {e}")
                        continue
                
                logging.info(f"   Page {page+1}/{total_pages} done. New: {new_count}")
                page += 1
                
            except Exception as e:
                logging.error(f"Critical error on {emp_name} page {page}: {e}")
                # –ï—Å–ª–∏ —É–ø–∞–ª–∏ - –Ω–µ –∏–¥–µ–º –¥–∞–ª—å—à–µ –ø–æ —ç—Ç–æ–π –∫–æ–º–ø–∞–Ω–∏–∏, –Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏–º —Ç–æ, —á—Ç–æ —É—Å–ø–µ–ª–∏ (–µ—Å–ª–∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –±—ã–ª–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π)
                break

        # –ö–û–ù–ï–¶ –ö–û–ú–ü–ê–ù–ò–ò
        producer.flush()
        if max_pub_date:
            state.update_state(emp_id, emp_name, max_pub_date)
            logging.info(f"‚úÖ {emp_name} Finished. State updated to {max_pub_date}")
        else:
            logging.info(f"üí§ {emp_name} No new vacancies.")
            
        logging.info("‚òï Coffee break 10s...")
        time.sleep(10)

    if producer:
        producer.close()

parser_task = PythonOperator(
    task_id='zen_parser_v10',
    python_callable=run_parser_v10,
    dag=dag
)